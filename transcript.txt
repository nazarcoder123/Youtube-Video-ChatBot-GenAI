~0~This is everything you need to know ~1~about prompt engineering. If you're not ~3~familiar with the term prompt ~4~engineering, it's a bunch of awesome ~7~strategies to get the most out of your ~9~artificial intelligence. So, Google put ~11~together this prompt engineering guide. ~13~I'm going to go over it. We're going to ~14~go more in depth and by the end you are ~17~going to be a master prompt engineer. So ~20~the very basics when you're chatting ~22~with a model like chat GPT, like Gemini, ~25~like Claude, you're typing in some input ~28~in natural language and then it gives ~30~you an output also usually in natural ~34~language. And the way that these models ~36~work is by taking your input, whatever ~38~your prompt is, and predicting what it ~41~thinks the output would be based on your ~43~prompt. So, your prompt really matters. ~46~How you structure it, what words you ~48~use, examples you give, all of these ~51~things are prompt engineering. And there ~54~are a number of triedand-true prompt ~56~engineering strategies which also depend ~58~on which model you're using, how good it ~60~is, what's the token limit, and we're ~62~going to go over all of that. So, ~64~remember how an LLM works. It's a ~66~prediction engine. The model takes ~68~sequential text as an input, your ~70~prompt, and then predicts what the ~72~following token should be based on the ~74~data it was trained on. And a token, if ~78~you're not familiar, is basically just a ~80~word. It's usually 3/4s of a word. We're ~83~not going to get into too much technical ~84~detail about it, but just think about ~85~it. Token equals word. And so the ~88~model's trying to predict the next token ~90~is added to the initial prompt and then ~92~it tries to predict the next token after ~95~that including based on your original ~97~prompt plus the additional token. And it ~100~does that over and over and over again ~103~until it thinks it has produced the ~105~proper output. Prompt engineering is the ~107~process of designing highquality prompts ~109~that guide LLMs to produce accurate ~112~outputs. That's a great definition for ~114~it. And if you've ever prompted a large ~116~language model, found that it didn't ~118~give you exactly what you wanted, ~120~changed your prompt, and then it did, ~122~that is prompt engineering. So before we ~124~get into the specific prompt engineering ~127~strategies, let me go over a few basic ~129~terms. Every large language model is ~132~different. Some are vastly different, ~134~some are a little bit different, and ~136~each of these differences matter. ~138~Setting these settings correctly will ~140~also help you get the most out of your ~142~prompts. So, first output length. Output ~145~length is the maximum number of tokens a ~147~model should output in response to your ~150~prompt. The longer the output length, ~152~the longer the response might be to your ~155~prompt. Now, if you just ask what's 2 ~158~plus 2, it's not going to spend ~160~paragraphs and paragraphs of text ~162~explaining to you the answer because the ~163~answer is simple and short. But if you ~165~ask it for code, this output length ~168~setting might make a big difference. ~170~Now, if the model's outputting more ~172~tokens, that potentially costs more ~173~money, takes longer, and uses more ~176~energy, more electricity. But here's an ~178~interesting nuance to keep in mind. ~181~Reducing the output length of the LLM ~183~doesn't cause the LLM to become more ~185~stylistically or textually succinct in ~188~the output it creates. Instead, it just ~191~causes the LLM to stop predicting more ~193~tokens once the limit is reached. If you ~195~set the output length really short, it's ~198~not going to give you shorter, more ~200~succinct answers. It's just going to ~203~stop outputting once it reaches that ~205~limit. Let me show you an example. So, ~207~here's Google AI Studio. Here is the ~209~output length setting right here. Right ~211~now, it's set to 65,000 tokens. Let's ~214~set that to five tokens. So, I say write ~217~a story about a panda bear. I'm using ~219~Gemma 327B which is a non-thinking model ~222~and it just says the bamboo whisperer ~225~doesn't even make sense and that is a ~227~maximum of five tokens. Now let's say we ~230~move it up to 50 tokens. Let's give that ~232~same prompt again and here we go. So the ~235~bamboo whisperer and Lynn was not like ~238~the other pandas at the Misty Peak ~240~Reserve and so on and then it finally ~242~just stops right here. He didn't just ~245~taste the So, that was a maximum of 50 ~248~tokens. And of course, if I set it to ~250~5,000 tokens, it's going to finish the ~252~story. But you can see here, it didn't ~254~write a succinct story. It just stopped ~257~outputting. So, that's really important ~258~to keep note of. The next setting is ~261~sampling controls. Now, here's the gist ~263~of how large language models predict the ~266~next token. LLMs do not formally predict ~269~a single token. rather LLMs predict ~272~probabilities for what the next token ~274~could be with each token in the LLM's ~277~vocabulary getting a probability. So if ~280~you say the cow jumped overthe', it ~283~doesn't just predict moon. It'll give a ~285~probability score to every single token ~288~in its vocabulary. And the one with the ~291~highest probability is the one that you ~293~see. And there are three main settings ~296~within these sampling controls. ~298~temperature, top K, and top P. And we're ~300~going to go over all three of those. ~302~Now, personally, I think temperature is ~304~probably the most important setting ~306~within sampling controls. Temperature ~308~controls the randomness in token ~311~selection. But really, a more simple way ~313~to think about it is the higher the ~315~temperature, the more creative or unique ~318~the responses will be. The lower the ~320~temperature, the less creative it'll be. ~323~So once again, write a story about a ~325~panda bear. We have temperature set to ~327~one. So that is the most creative. Let's ~329~give it a try. Okay, so we got an ~331~output. The rain in bamboo forest wasn't ~333~just a drizzle, it was a deluge. So ~336~let's stop there and let's give the same ~338~prompt again. Let's see what happens. So ~340~the same exact model, the same exact ~342~prompt. And as you can see, a completely ~346~different story. Now what happens if I ~348~set the temperature to zero? Let's see. ~350~I'm going to give it the same prompt ~351~again. Write a story about a panda bear. ~353~The bamboo forest hummed with the gentle ~355~rustle of leaves and the chirping of ~357~unseen birds. Great. Now let's do it ~360~again with temperature zero. And look at ~363~that. The bamboo forest hummed with the ~365~low thrum of cicas. So very similar, ~368~very similar to the previous one with a ~370~temperature of zero. And that's what ~372~you're going to get. You're going to get ~373~very different responses every single ~375~time you give the same prompt when the ~377~temperature is really high and the ~378~opposite when it's very low. You're ~380~going to get very consistent responses. ~382~So remember to adjust your temperature ~384~dependent on the use case that you are ~387~doing. All right. Next, top K and top P. ~390~Now, to be honest, I don't really use ~392~top P and top K settings at all. I use ~394~temperature, and that's enough for me. ~396~But let me just briefly go over what ~398~they are. So top K works very similarly ~401~to temperature. Top K sampling selects ~403~the top K most likely tokens from the ~406~model's predicted distribution. And the ~408~predicted distribution is just the ~410~vocabulary set and the assigned ~412~probabilities. So the higher the top K, ~414~the more creative and varied the model's ~415~output. The lower, the more restive and ~417~factual. So very similar, just another ~420~setting to play around with for ~422~creativity. Top P sampling is a way to ~425~limit the set of vocabulary that the ~427~model will choose from based on its ~429~cumulative probability. And don't worry ~431~if you don't understand that. Honestly, ~432~I rarely ever use top P. It's just good ~435~to kind of generally know what it does. ~436~So play around with those three ~438~settings. Now in this document, they ~440~give suggested starting points for all ~442~three of these settings. And generally ~445~within whatever chat interface you're ~447~using, they give you their default ~449~settings as well. But for here, as a ~451~general starting point, a temperature of ~453~0.2, that's actually a lot lower than I ~455~usually start at. I usually start at ~457~like 6, top P of 0.95, and top K of 30 ~460~will give you relatively coherent ~462~results that can be creative, but not ~464~excessively. So, if you want more ~466~creative results, raise the top P and ~468~raise the top K. And obviously, if you ~471~want more consistent, less creative ~473~results, do the opposite. Drop those ~474~settings down. All right. Now, what you ~476~came here for, prompting techniques. ~480~Now, the first thing we're going to talk ~481~about is general prompting or zero shot. ~484~If you've heard of zero shot, it doesn't ~486~mean giving the model one try. It's a ~489~little confusing in that way. Zero shot. ~491~The shot term means how many examples ~494~are you giving the model. So with zero ~497~shot, it's the simplest type of ~498~prompting. You're not giving the model ~500~any examples of what you want as the ~502~output. So with zero shot, you're ~505~basically just providing a description ~507~of the task you want accomplished. ~509~Whether that's write a story, do some ~510~math, write some code, but nothing else. ~513~Just a thorough description of what ~515~you're looking for. Typically, the more ~517~examples you need, the more complex the ~520~task that you're asking the large ~522~language model to accomplish for you. ~524~So, if it's a pretty simple task, like ~526~writing a story, you probably don't need ~528~a bunch of examples. So, here's an ~531~example of a zeros prompt. We are asking ~534~the model to classify movie reviews as ~537~positive, neutral, or negative. And ~539~here's the review. Her is a disturbing ~542~study revealing the direction humanity ~544~is headed. If AI is allowed to keep ~546~evolving unchecked, I wish there were ~547~more movies like this masterpiece. ~549~Sentiment colon. So that's what we're ~551~asking for. What is the sentiment? And ~553~use positive, neutral, negative. And it ~555~got it right. Perfect. Now for more than ~557~zero shot, one shot or few shot that ~560~just means giving the large language ~562~model more examples to work with. One ~564~shot means giving it one example. Few ~566~shot means two or more. Now with one ~569~shot, the model's going to try its best ~571~to mimic whatever example you give. And ~574~when you use fshot, you're giving the ~576~model more chance to get the desired ~578~pattern correct from your example. So if ~581~you want the output in a specific ~582~format, that's a great way to do it. The ~584~number of examples you need for fshot ~586~prompting depends on a few factors ~588~including the complexity of the task, ~590~which we talked about, the quality of ~592~the examples. If you have great examples ~594~to give, then that just makes the job of ~596~the large language model that much ~598~easier. and the capabilities of the ~600~generative AI model you are using. As a ~604~general rule of thumb, you should use at ~605~least three to five examples for fuse ~607~shop prompting. So let's look at the ~609~example they give in this document. The ~611~goal is to parse pizza orders to JSON. ~614~Here it is. So here's the prompt. Parse ~616~a customer's pizza order into valid ~618~JSON. Example, I want a small pizza with ~622~cheese, tomato sauce, and pepperoni. Now ~625~if we give that exact prompt with ~627~nothing else to the model, the model is ~630~going to infer what the structure of the ~634~JSON object should be. And the problem ~636~with that is if we do this a thousand ~639~times with a thousand different orders, ~640~we might get different JSON object ~643~structures. But if we provide an example ~646~like here, we want size, type, and ~649~ingredients. And within ingredients, we ~652~want an array of ingredients. Then the ~654~model is going to be much more likely to ~656~use this exact structure. And here's a ~659~continuation of that same prompt. ~661~Example, can I get a large pizza with ~663~tomato sauce, basil, and mozzarella? So, ~665~size type ingredients right there. Now, ~668~I would like a large pizza with the ~671~first half cheese and mozzarella, and ~673~the other tomato sauce, ham, and ~674~pineapple. JSON response. This is where ~676~we're asking for the output from the ~678~model. So size, type, ingredients, and ~681~it's the same exact structure that we ~683~used in the examples. So this is a great ~686~use case for fshot prompting. All right, ~689~next let's talk about the system ~690~message, contextual prompting, and role ~693~prompting. The gist of what these things ~695~do is essentially getting the model to ~698~act as some role. So act as a senior ~703~developer, act as a CEO, act as a ~706~teacher. As soon as you give it that ~708~role description, the model will start ~710~taking on the behaviors and qualities of ~713~whatever it thinks that role would be. ~715~So first system prompting it sets the ~717~overall context and purpose for the ~719~language model. It defines the big ~721~picture of what the model should be ~722~doing like translating a language, ~724~classifying a review, etc. So if you ~726~look at Google AI Studio, there's this ~728~little clipboard right here and it says ~730~system instructions. That's the same ~732~thing as system message. If you click ~733~it, this is where it says optional tone ~736~and style instructions for the model, ~737~which is where you would describe kind ~739~of the overall theme of what you're ~741~trying to accomplish. Next is contextual ~744~prompting. Contextual prompting provides ~746~specific details or background ~747~information relevant to the current ~749~conversation or task. It helps the model ~751~to understand the nuances of what's ~753~being asked and tailor the response ~755~accordingly. So in the document, they ~757~give this example. Here's the context, ~759~contextual prompting context. You are ~762~writing for a blog about retro880s ~764~arcade video games. Then the actual task ~767~suggests three topics to write an ~769~article about with a few lines of ~770~description of what this article should ~773~contain. So in the actual task, it ~775~doesn't say that you need to write a ~777~blog for a retro80s arcade video games, ~780~but that's in the context. So if we run ~782~it, here we go. The Unsung Heroes 5 from ~785~Pixels to Power Coin Op Culture Clash. ~788~So you're separating the context from ~791~the actual task at hand. And last, role ~794~prompting. Role prompting assigns a ~796~specific character or identity for the ~798~language model to adopt. This helps the ~800~model generate responses that are ~802~consistent with the assigned role and ~803~its associated knowledge and behavior. ~806~Where I see this most of all is in a ~809~Gentic framework, specifically Crew AI. ~812~And Crew AI actually does a good job of ~814~taking this a step further. and they ~817~have a bunch of data to support doing ~819~this. And this strategy works just the ~821~same in direct large language model ~823~prompting. So look at this. Here's a ~825~role attribute that you can use in your ~828~agent definition. Defines the agents ~830~function and expertise within the crew. ~832~It also has the goal or the task, the ~835~individual objective that guides the ~837~agents decision-making. And it even has ~839~a backstory provides context and ~841~personality to the agent enriching ~843~interactions. So role prompting is very ~847~very powerful. Now here's another ~849~example from the document. I want you to ~851~act as a travel guide. I will write to ~853~you about my location and you will ~855~suggest three places to visit near me. ~857~So the actual role is travel guide and ~860~the task is suggest three places to ~862~visit. So here's my suggestion. I'm in ~865~Amsterdam and I want to visit only ~867~museums travel suggestions. Let's run ~869~it. And here we go. So I'm ready to be ~871~your Amsterdam museum guide based on ~873~your request. Here are three museum ~875~suggestions. So that is when roll ~877~prompting becomes really powerful. All ~878~right. Next is one I had not actually ~880~heard of. It's called step back ~882~prompting. And it's akin to a few other ~885~prompting techniques that I know of, but ~887~it's kind of unique. Let me show it to ~889~you. So step back prompting asks the ~891~model to first consider a general ~894~question related to the specific task at ~896~hand and then feeding the answer to that ~899~general question into a subsequent ~901~prompt for the specific task. If that ~904~sounds confusing, I'll show you an ~905~example in a moment. So, what is the ~907~point of that? It allows the LLM to ~910~activate relevant background knowledge ~912~and reasoning processes before ~913~attempting to solve the specific ~915~problem. By considering the broader and ~918~underlying principles, LLMs can generate ~920~more accurate and insightful responses. ~923~It encourages the LLMs to think ~925~critically and apply their knowledge in ~927~new and creative ways. It changes the ~929~final prompt doing the task by utilizing ~931~more knowledge in the LLM's parameters ~933~than would otherwise come into play when ~935~the LLM is prompted directly. So ~937~fascinating. All right, so first here is ~940~an example default prompt. This is not ~942~step back prompting. So write a one ~945~paragraph story line for a new level of ~947~a first-person shooter video game that ~949~is challenging and engaging. And ~950~remember this is standard prompting, not ~952~step back. Let's hit enter and see. ~954~Okay, so the ravaged derelic space ~957~station Icarus has become a deadly ~959~labyrinth, a colossal gravitational ~961~anomaly, has ripped the station apart ~963~and so on. So pretty good. And what it ~966~says here is very interesting and ~967~something that I come across all the ~969~time. Whenever I'm asking a model to do ~971~something really creative for me, it ~973~tends to give me like really generic ~975~responses, even when I set the ~976~temperature to one. So creative writing ~979~is one of the things that I actually ~981~least go to large language models for. ~983~So going forward, I'm going to try the ~985~stepback method. So now instead, let's ~987~get it to think about this topic space ~990~before we actually give it the assigned ~992~task. So based on popular first-person ~995~shooter action games, what are five ~996~fictional key settings that contribute ~998~to a challenging and engaging level ~1000~story line in a first-person shooter ~1002~video game? All right, so it gave me a ~1004~very robust answer of five fictional key ~1007~settings. So now I copied all of this ~1011~output and I said here's the context and ~1013~I pasted it in. And at the very bottom I ~1015~say take one of the themes and write a ~1018~one paragraph story line for a new level ~1021~of a firstperson shooter video game that ~1023~is challenging and engaging. And let's ~1025~see what it does. The player finds ~1027~themselves in the heart of the derelik ~1030~space station having just emerged from a ~1032~terrifying zerog section filled with ~1034~swarms of rapidly evolving nanobots. So ~1037~just a good way to get more accuracy and ~1038~more breath of knowledge from your large ~1041~language model. All right, next for the ~1044~prompting technique that really changed ~1046~large language models completely. Chain ~1048~of thought. Now before I get into this, ~1052~chain of thought is being built into ~1054~many of these models today. Anytime you ~1057~hear about test time compute or ~1059~inference time compute, this is what ~1061~they're talking about. The models output ~1063~their thinking in the form of chain of ~1066~thought in the thinking section of its ~1067~output before giving you the actual ~1069~output. But before these were built into ~1072~the models, we were prompting the models ~1074~to do this in the kind of standard ~1076~output. And it's actually quite simple ~1079~but really powerful. Now, if you ~1081~remember back to my model benchmarks ~1083~from, let's say, even 6 months or a year ~1085~ago, I would append all of my prompts ~1088~with think step by step and show your ~1092~work step by step. And just by adding ~1094~that to the prompt, we got much better, ~1097~much more accurate, higher quality ~1099~outputs from the model. Now, as I said, ~1102~a lot of models come with this baked in, ~1104~but not all of them. any model that is ~1106~on the smaller side or doesn't have a ~1108~thinking mode or test time compute mode, ~1111~this is still a very powerful prompting ~1113~method. Now, it's funny because most ~1116~recent models actually do this by ~1118~default, even if they're not a quote ~1120~unquote thinking model. Let's take a ~1122~look. So, this is Gemini 2.0 Flash ~1124~Light. This is not a thinking model. ~1127~Here's the prompt. When I was 3 years ~1129~old, my partner was three times my age. ~1131~So, three versus nine. Now I am 20 years ~1135~old. How old is my partner? So here's ~1138~how to solve the problem. Find the ~1140~partner's age. So it is thinking step by ~1143~step. And the answer is 26. And that is ~1147~correct. Now if we look at the example ~1149~in the document, now the output did not ~1152~show its work and thus it got 63 years ~1154~old and so that's wrong. But when they ~1157~asked think step by step, it actually ~1160~output each step of the thinking and was ~1162~able to get the correct response. And so ~1165~anytime you have a smaller model or an ~1167~older model, and by the way, you should ~1169~still use those models for specific use ~1171~cases like when inference speed is ~1173~important, when cost is important, there ~1176~are many considerations when choosing ~1178~the right model. And you can get a lot ~1180~out of these smaller, let's say, less ~1182~intelligent models simply by using some ~1184~of these prompting strategies. And not ~1186~only that, you can combine prompting ~1189~techniques. So you can take one shot or ~1191~few shot prompts and mix it with chain ~1193~of thought. Let's look at that example. ~1195~So question, when my brother was 2 years ~1197~old, I was double his age. Now I'm 40. ~1199~How old is my brother? Let's think step ~1201~by step. And then we give it an example. ~1203~Here's an answer. When my brother was 2 ~1205~years old, I was 2 * 2 equals 4 years ~1208~old. That's an age difference of 2 years ~1210~and I am older. Now I am 40. So, my ~1213~brother is 40 - 2, 38 years old, the ~1216~answer is 38. Then, let's ask the ~1218~question. When I was three, my partner ~1219~was three times my age. Now I'm 20. How ~1221~old is my partner listening step by ~1223~step? And it basically mimicked the ~1226~exact same thinking as the example we ~1228~gave it. And chain of thought is really ~1230~powerful for a lot of different use ~1232~cases. Basically, anything under the ~1235~category of STEM, science, technology, ~1237~engineering, and math, chain of thought ~1239~is very, very powerful. but also logic ~1241~and reasoning. And there are just so ~1244~many different categories in which ~1245~letting the model think step by step ~1247~improves their output greatly. All ~1249~right, next let's talk about ~1251~self-consistency, which is another very ~1253~powerful prompting technique. So LLM's ~1256~ability to reason is often seen as a ~1258~limitation that cannot be overcome ~1260~solely by increasing model size. The ~1263~model can be prompted to generate ~1264~reasoning steps like a human solving a ~1266~problem. However, Chain of Thought uses ~1269~a simple greedy decoding strategy, ~1271~limiting its effectiveness. And greedy ~1274~decoding just means picking whichever ~1276~token is the highest probability token. ~1279~And according to the document, it limits ~1281~its effectiveness. Now, in comes ~1284~self-consistency. Self-consistency ~1287~combines sampling and majority voting to ~1290~generate diverse reasoning paths and ~1292~select the most consistent answer. What ~1294~that basically means is running the same ~1296~prompt against the model, let's say, ~1299~five different times, and then using the ~1301~model to vote on whichever one it thinks ~1304~is the right answer or the best ~1306~solution. And it improves the accuracy ~1309~and coherence of responses generated by ~1311~large language models. All right, so ~1312~let's look at an example. We have a task ~1316~to classify emails by either important ~1318~or not important. And here's the email. ~1320~Hi, I have seen you use WordPress for ~1322~your website. a great open source ~1324~content management system. I have used ~1325~it in the past and so on. Here's the ~1327~important part. I did notice a bug in ~1329~the contact form which happens when you ~1331~select the name field. And so this would ~1333~be an important email. Somebody's ~1335~contacting you about a potential bug. So ~1337~classify the above email as important or ~1339~not important. Let's think step by step ~1341~and explain why. So here's the output. ~1344~Step one, identify the purpose. The ~1346~purpose of the email is to inform the ~1347~recipient of a bug in the contact form. ~1350~Step two, assess the potential impact of ~1352~the bug. Step three, consider the ~1354~credibility of the sender. Conclusion, ~1356~important. Great. Based on the potential ~1359~impact of the bug and the credibility of ~1361~the sender, the email should be ~1362~classified as important. The recipient ~1363~should take immediate action to fix the ~1365~bug and protect their website from ~1366~attack. Now, let's look at the second ~1369~output. So again, we're prompting the ~1372~model multiple times and then we're ~1374~going to have the model decide which ~1376~output is best. So output two, lack of ~1379~urgency, non-critical bug report, lack ~1382~of personal impact. Absence of action ~1385~request and sender's intent. Conclusion, ~1388~not important. And for attempt three, it ~1391~deems it important. So two out of three ~1394~outputs from the large language model ~1396~deemed it important. And so you can do ~1398~this three times, you could do this five ~1400~times, you could do this 50 times. And ~1402~then you basically take whichever output ~1404~or whichever response happened most ~1407~often as the truth or as whatever the ~1410~best response is. So in this case two ~1412~out of three were important. Let's ~1414~classify it as important. But it comes ~1416~at a big cost. Obviously if you're ~1418~running these prompts multiple times for ~1420~every single task. There's a high cost, ~1423~there's high latency. And so just ~1425~something to keep in mind as you're ~1427~deciding the trade-offs of using this ~1428~prompting strategy. So now that we're ~1430~familiar with chain of thought and ~1432~self-consistency, let's talk about tree ~1434~of thoughts. So it allows LLM to explore ~1438~multiple different reasoning paths ~1439~simultaneously rather than just ~1441~following a single linear chain of ~1443~thought. Here's what it looks like. So ~1445~here is chain of thought, input, the ~1447~different steps, and then the output. ~1449~But here is tree of thought. you have an ~1451~input and then it tests at each step ~1455~different outputs that lead to the next ~1457~set of outputs that finally lead to the ~1460~final output. And this is using a ~1462~combination of self-consistency and ~1465~chain of thought. So you can imagine it ~1467~like this. We have an input, it comes up ~1469~with the first step, multiple first ~1472~steps, and then you have it decide which ~1474~one is most accurate or best. Then it ~1477~goes on to the next one and does it ~1478~again and again and again until you ~1480~finally get the final output. Now with ~1483~tree of thought, doing this strictly ~1486~between just a user and what you can ~1488~type into a prompt box to the model is ~1491~not really viable. There's just too much ~1493~going on. You really need to implement ~1495~tree of thought with code or use some ~1497~kind of framework to do it for you. And ~1499~this approach makes tree of thought ~1501~particularly well suited for complex ~1504~tasks that require exploration. And so ~1506~if you have more complex tasks, more ~1509~sophisticated tasks, tree of thought ~1511~might be great for you. All right, next ~1513~let's go over react. That is reason and ~1517~act. Reason and act prompting is a ~1519~paradigm for enabling large language ~1521~models to solve complex tasks using ~1523~natural language reasoning combined with ~1525~external tools, search, code ~1527~interpreter, etc. tools are incredibly ~1531~important for taking the raw ~1532~intelligence of a large language model ~1535~and allowing it to accomplish real world ~1538~tasks. React mimics how humans operate ~1541~in the real world. You can kind of think ~1542~of React as agents. Basically, you have ~1545~the logic, the core large language ~1548~model, and then you give it tools. Tools ~1550~to get new knowledge or tools to save ~1553~memories or tools to communicate with ~1556~other agents. Basically, anything you ~1557~want. React prompting works by combining ~1560~reasoning and acting into a thought ~1562~action loop. The LLM first reasons about ~1564~the problem and generates a plan of ~1565~action. Then it performs the actions in ~1567~the plan and observes the results. Now, ~1570~if this sounds familiar, a lot of the ~1573~frontier models nowadays have this built ~1575~in. You can check on and off different ~1578~tools, and it has chain of thought built ~1581~into its thinking mode. So, here's ~1583~Gemini 2.5 Pro preview as of May 6th. ~1588~And here are its tools, structured ~1590~output, code execution, function ~1592~calling, Google search. And so, this is ~1595~a perfect example of React being done ~1598~for you. But, of course, when you're ~1601~using the cutting edge models, you're ~1603~also paying the most and you're probably ~1605~waiting the longest. So, highest cost, ~1607~highest latency. But you can get a lot ~1609~of these benefits on older or let's just ~1612~say smaller, faster, less intelligent ~1615~models simply by using the React ~1618~framework. So React is really just an ~1621~agent. And in this example, you're ~1622~seeing the very basic version of this. ~1624~So here's some Python code. We're ~1626~loading lang chains agents. Here's the ~1628~prompt. How many kids did the band ~1630~members of Metallica have? Now here's ~1632~the LLM. We're using Vertex AI, which is ~1634~Google's product. and we're using the ~1636~tool SER API which is a Google search ~1639~API giving search web search available ~1642~as a tool to the models. So we run it ~1645~and that's fine. Now given these few ~1648~lines of code we've now given the LLM ~1650~the ability to plan to execute to review ~1655~what happened and then execute again if ~1657~it needs to. So here's the output. ~1659~Metallica has four members. So let's do ~1662~a search. Here's a tool called search ~1664~action input. How many kids does James ~1667~Hetfield have? Observation. Three ~1669~children. Thought Metallica band members ~1671~have three children. Let's do another ~1673~search. Lars or Kirk Hammet and so on ~1677~all the way down. So, it does one search ~1680~per band member, finds the total number ~1682~of kids, adds them all together, and ~1684~that's the answer. And yeah, this is ~1686~really just agents. And nine times out ~1689~of 10, there's no reason to write the ~1691~agentic framework yourself. You can take ~1694~lang chain, you can take crewi, and they ~1698~do fantastic jobs of putting these ~1700~frameworks together for you. All right, ~1702~so all of these prompting techniques can ~1704~obviously get very complex and tedious ~1707~and take a long time for you to write ~1709~manually. But what if you can have AI ~1711~write the prompts for you? This is ~1713~something I do all the time. This is ~1715~called automatic prompt engineering. Let ~1718~me tell you how I do this. So I'm ~1720~frequently asking large language models ~1722~to write code for me. But I also don't ~1725~want to write a very detailed PRD, which ~1729~is just a list of requirements for the ~1731~code that I want written. So what I do ~1733~is I'll just come up with a few sentence ~1735~description of what I want built. Then I ~1738~will ask the model to write up a PRD for ~1740~me. Then I take that PRD, put it back ~1743~into another model, and I say, write ~1745~code based on this PRD, and so it does ~1748~the job of writing extensive detail for ~1752~whatever I want built. So it's a good ~1755~way to add a lot more detail to whatever ~1757~prompt you're writing. But you can not ~1759~only do that, you could take any of the ~1761~prompting techniques we've talked about ~1762~today and simply say, "Here's my most ~1765~basic prompt. Do chain of thought with ~1767~it or do self-consistency with it." and ~1769~it will write the prompt that you can ~1771~then put back into the large language ~1773~model to actually do that prompting ~1774~technique. All right, so next I want to ~1776~talk about a prompting technique that I ~1778~use that I'm not even sure what it's ~1780~called. But it's basically deciding when ~1782~to ask the model to write and execute ~1785~code for the prompt for the solution ~1788~versus just give you the natural ~1789~language solution. All right, so let's ~1791~use a prompt test that I used to do all ~1792~the time, but now all the models get it ~1794~right. So this is GPT40 and it says how ~1796~many Rs are in the word strawberry. how ~1799~it did get it right. There are three Rs ~1801~in the word strawberry, but frequently ~1803~other models would get this wrong. So, ~1805~here's a different way to think about it ~1806~where you're always going to get the ~1808~right answer. Many models have the ~1810~ability to write and execute code. So, ~1812~rather than just me saying how many Rs ~1814~are in the word strawberry, I'm going to ~1816~explicitly tell it to write code to ~1818~count the number of Rs in a given word ~1820~starting with the word strawberry. Now, ~1822~if you write code that takes an input of ~1824~a word and can count the number of Rs ~1827~within that word, it's always going to ~1829~be right as long as the code is right. ~1830~And for use cases like this, the model's ~1833~ability to write code is actually much ~1835~greater than its ability to answer these ~1837~questions right without it. So, here's ~1839~an example. Write code to count the ~1841~number of Rs in a given word. Starting ~1843~with the word strawberry, execute that ~1845~code. And if we open up the analysis, so ~1848~define the word and it wrote code and ~1851~here's the output. And so it actually ~1854~wrote code and executed code and now I ~1856~know it's going to be accurate. So I ~1858~call that prompting using code. All ~1861~right, to wrap this up, let's talk about ~1863~some best practices. Number one, provide ~1865~examples. We talked about zeroot, one ~1867~shot, and few shot. If you can, when you ~1870~can, try to give your model examples, ~1872~especially when you're trying to get ~1874~consistent outputs. Design with ~1876~simplicity. I really agree with this. ~1878~Start with simple prompting and only add ~1881~more instructions or more nuanced ~1883~instructions when absolutely necessary. ~1885~And anytime you're writing a task or a ~1888~prompt for the model, try to think, is ~1889~this the simplest version of what I'm ~1892~asking for? Be specific about the ~1894~output. This is really important. If ~1896~you're expecting JSON, say you're ~1898~expecting JSON. If you're expecting ~1901~every letter of the first word of the ~1903~output to be the letter B, make sure you ~1905~say that. Whatever it is, specify your ~1908~output because otherwise, the model's ~1910~just going to try to guess what you're ~1911~asking for. Use instructions over ~1913~constraints. So, an instruction provides ~1916~explicit instructions on the desired ~1918~format, style, or content of the ~1920~response. A constraint sets the ~1923~limitations or boundaries. So rather ~1925~than saying here's what not to do, say ~1928~here's what to do. Next, control the max ~1931~token length. This is not something I do ~1933~actively, but especially for higher ~1936~scale production use cases, this is ~1938~really important for optimizing the ~1940~latency and the cost. And another note ~1942~on high scale or production use cases is ~1945~use variables. Use variables in the ~1947~prompt. So here's an example. Here are ~1948~the variables. City Amsterdam prompt, ~1950~you are a travel guide. Tell me a fact ~1953~about the city. city and then going ~1955~forward you can programmatically insert ~1957~any city you want here. The last thing ~1959~I'm going to suggest is stay up to date ~1961~on all of these models, what their ~1963~capabilities are, what the limitations ~1965~are because that's going to help you ~1967~know how to format your prompts most ~1969~effectively to get what you're looking ~1971~for. Obviously follow my channel if ~1973~you're not already subscribed because ~1975~that's what I do all day every day. ~1977~Hopefully I can keep you informed. I ~1979~also have a newsletter that goes over ~1980~the same thing for future.ai. I want to ~1984~thank Google and specifically the author ~1985~of this document, Lee Bonstra, for this ~1988~prompt engineering guide. It's ~1990~fantastic. Check it out. I'll drop a ~1992~link in the description below. If you ~1993~enjoyed this video, please consider ~1995~giving a like and subscribe. and I'll ~1997~see you in the next 